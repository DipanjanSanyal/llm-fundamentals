{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "Training is originally run in Kaggle. It should work in other environments as well, except for how the secret token is used.\n",
    "\n",
    "Firstly we import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:15.343188Z",
     "iopub.status.busy": "2025-06-09T05:22:15.342884Z",
     "iopub.status.idle": "2025-06-09T05:22:32.042415Z",
     "shell.execute_reply": "2025-06-09T05:22:32.041574Z",
     "shell.execute_reply.started": "2025-06-09T05:22:15.343160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datasets\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-04T09:59:21.895807Z",
     "iopub.status.busy": "2025-06-04T09:59:21.895466Z",
     "iopub.status.idle": "2025-06-04T09:59:21.905567Z",
     "shell.execute_reply": "2025-06-04T09:59:21.904708Z",
     "shell.execute_reply.started": "2025-06-04T09:59:21.895778Z"
    }
   },
   "source": [
    "### Configurations\n",
    "We shall try to keep the minimalistic configuration which we tried in pure pytorch version [here](https://github.com/DipanjanSanyal/llm-fundamentals/blob/main/Simplified%20GPT%20Pretraining%20-%20Pure%20Pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:32.044189Z",
     "iopub.status.busy": "2025-06-09T05:22:32.043746Z",
     "iopub.status.idle": "2025-06-09T05:22:32.050085Z",
     "shell.execute_reply": "2025-06-09T05:22:32.049291Z",
     "shell.execute_reply.started": "2025-06-09T05:22:32.044168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "context_length = 64\n",
    "embed_dim = 128\n",
    "heads = 4\n",
    "layers = 3\n",
    "batch_size = 32\n",
    "learning_rate = 0.00003\n",
    "response_tokens = 50\n",
    "num_epochs = 3\n",
    "output_model_name = 'wikipedia_sample_tiny_gpt2_base'\n",
    "commit_message = \"1st commit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Here we are going to use much easier coding with the virtue of Huggingface libraries. Hence, let us try to use a better dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:32.051358Z",
     "iopub.status.busy": "2025-06-09T05:22:32.051056Z",
     "iopub.status.idle": "2025-06-09T05:22:34.103659Z",
     "shell.execute_reply": "2025-06-09T05:22:34.102975Z",
     "shell.execute_reply.started": "2025-06-09T05:22:32.051332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading small wikipedia dataset from huggingface hub\n",
    "data = datasets.load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", streaming = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming only sets it up, does not download the data until called. We shall be calling only a subset of data.\n",
    "\n",
    "We shall not use very short texts so we'll exclude them from getting downloaded.\n",
    "\n",
    "Also, we'll limit the dowbload to a desired number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:34.104734Z",
     "iopub.status.busy": "2025-06-09T05:22:34.104461Z",
     "iopub.status.idle": "2025-06-09T05:22:48.517650Z",
     "shell.execute_reply": "2025-06-09T05:22:48.516739Z",
     "shell.execute_reply.started": "2025-06-09T05:22:34.104708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This is not reproducible through\n",
    "# To make it reproducible, hashing can be used, but that won't be efficient\n",
    "# Otherwise, one can use streaming = False in the above cell with split = 'train[:5%]' although not optimum on disk\n",
    "\n",
    "def filter_and_limit(dataset, min_char_len, row_subset):\n",
    "    result = []\n",
    "    for entry in dataset['train']:\n",
    "        if (len(entry['text']) > min_char_len):\n",
    "            result.append(entry)\n",
    "        if len(result) >= row_subset:\n",
    "            break\n",
    "    return Dataset.from_list(result)\n",
    "\n",
    "\n",
    "sample_data = filter_and_limit(data, 1024, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:48.519918Z",
     "iopub.status.busy": "2025-06-09T05:22:48.519582Z",
     "iopub.status.idle": "2025-06-09T05:22:48.525506Z",
     "shell.execute_reply": "2025-06-09T05:22:48.524507Z",
     "shell.execute_reply.started": "2025-06-09T05:22:48.519886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, unlike the pure pytorch version, here we shall not be streaming all the texts to a single long string but will keep them separate. This will ensure that every selected example contains tokens from same wikipedia article. This is because -\n",
    "1. we are working with a small context window, and we are fine to limit the context within one paragraph\n",
    "2. streaming would have resulted into samples with texts from 2 different articles which can be prevented if we do so\n",
    "3. here we shall be passing the whole training data <code>num_epochs</code> times rather than sampling, #2 cases would defnitely arise if we stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Tokenization, Chunking, Input vs Target Setup\n",
    "1. Firstly, we import an instance of a pre-trained tokenizer class. This time we won't be using the tiny tokenmonsters tokenizers but will be using some standard tokenizer, say distilbert.\n",
    "2. Now we need to pass our data through the instance of this class using [this calling protocol](https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__) (similar to forward in neural nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:48.526788Z",
     "iopub.status.busy": "2025-06-09T05:22:48.526495Z",
     "iopub.status.idle": "2025-06-09T05:22:49.581884Z",
     "shell.execute_reply": "2025-06-09T05:22:49.581005Z",
     "shell.execute_reply.started": "2025-06-09T05:22:48.526769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# example call\n",
    "mysample = tokenizer(\n",
    "    sample_data['text'][0:3], # 3 paragraphs entered\n",
    "    truncation = True, # specifies that each sample will be tried to be split into disjoint chunks of max_length\n",
    "    max_length = context_length,\n",
    "    return_overflowing_tokens = True, # keeps track of which chunk was sampled from which sample\n",
    "    return_length = True, # returns number of tokens in the selected chunks\n",
    "    add_special_tokens = False # prevents adding special tokens to the tokenizer while applying the tokenizer\n",
    ")\n",
    "\n",
    "for k in mysample.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:49.582868Z",
     "iopub.status.busy": "2025-06-09T05:22:49.582638Z",
     "iopub.status.idle": "2025-06-09T05:22:49.588825Z",
     "shell.execute_reply": "2025-06-09T05:22:49.587817Z",
     "shell.execute_reply.started": "2025-06-09T05:22:49.582851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(mysample['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just based on 3 articles, we produced 256 disjoint chunks of 64 tokens each.\n",
    "\n",
    "By default, the chunking results into disjoint chunks, so the last chunk will mostly be shorter (inspect <code> mysample['length']</code>\n",
    "\n",
    "But we need chunks exactly as long as context_length. So, we'll exclude shorter chunks. We shall also use stride (overlap between chunks) to mimic somewhat same of what we had done in pure pytorch version. We chose to use 1/4th of the chunk overlapping with next chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:22:49.590618Z",
     "iopub.status.busy": "2025-06-09T05:22:49.589923Z",
     "iopub.status.idle": "2025-06-09T05:24:31.858473Z",
     "shell.execute_reply": "2025-06-09T05:24:31.857583Z",
     "shell.execute_reply.started": "2025-06-09T05:22:49.590589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation = True,\n",
    "        max_length = context_length,\n",
    "        stride = int(context_length*1/4),\n",
    "        return_overflowing_tokens = True,\n",
    "        return_length = True,\n",
    "        add_special_tokens = False\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "# Tokenizing and Chunking the whole dataset to chunks of length exactly equal to context_length\n",
    "tokenized_datasets = sample_data.map(tokenize, batched = True, remove_columns = data[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:31.859626Z",
     "iopub.status.busy": "2025-06-09T05:24:31.859366Z",
     "iopub.status.idle": "2025-06-09T05:24:31.865280Z",
     "shell.execute_reply": "2025-06-09T05:24:31.864576Z",
     "shell.execute_reply.started": "2025-06-09T05:24:31.859607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the train-test split since this data doesn't come with a default split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:31.866225Z",
     "iopub.status.busy": "2025-06-09T05:24:31.866015Z",
     "iopub.status.idle": "2025-06-09T05:24:32.084920Z",
     "shell.execute_reply": "2025-06-09T05:24:32.084054Z",
     "shell.execute_reply.started": "2025-06-09T05:24:31.866209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size = 0.2)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use data collator which is a function callable under <code>DataLoader</code> object. This is actually redundant for our task. But the code is written with generalization of different tasks in mind, so we need to apply this. \n",
    "\n",
    "For us it only creates input (X) vs target (y) inside a selected batch while training. It puts the same sequence in both X and y but when the data_collator is called during training, the chunk is split properly to X and y. \n",
    "\n",
    "But it is capable of handling unequal length sequences in a batch with padding, which we don't need to use since we are using all equal length chunks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:32.086081Z",
     "iopub.status.busy": "2025-06-09T05:24:32.085793Z",
     "iopub.status.idle": "2025-06-09T05:24:46.377525Z",
     "shell.execute_reply": "2025-06-09T05:24:46.376911Z",
     "shell.execute_reply.started": "2025-06-09T05:24:32.086056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data collator function for dynamic padding by batch within training loop\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) # By default it fits with masked language model (like BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:46.379021Z",
     "iopub.status.busy": "2025-06-09T05:24:46.378369Z",
     "iopub.status.idle": "2025-06-09T05:24:46.411834Z",
     "shell.execute_reply": "2025-06-09T05:24:46.411053Z",
     "shell.execute_reply.started": "2025-06-09T05:24:46.379001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Examining samples\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization\n",
    "\n",
    "We shall be using GPT2 architecture which is conveniently available in the hub, so we dont need to write all the model classes using Pytorch. Just to be clear, we are not loading the model weights which is usually done by <code>AutoModel.from_pretrained()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:46.413563Z",
     "iopub.status.busy": "2025-06-09T05:24:46.413318Z",
     "iopub.status.idle": "2025-06-09T05:24:47.734265Z",
     "shell.execute_reply": "2025-06-09T05:24:47.733389Z",
     "shell.execute_reply.started": "2025-06-09T05:24:46.413545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading only the model architecture and hyperparameters, not the trained weights\n",
    "# Also, customizing with desired transformer size (similar to pure pytorch)\n",
    "# This may not work well, because here our data and tokenizers are much larger, but let's still use it\n",
    "\n",
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size = len(tokenizer),\n",
    "    n_ctx = context_length,\n",
    "    n_positions = context_length,\n",
    "    n_embd = embed_dim,\n",
    "    n_head = heads,\n",
    "    n_layer = layers,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This is the part which is going to be heavy on compute.\n",
    "\n",
    "We'll now handle some housekeeping stuff (like logins) and then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:47.737236Z",
     "iopub.status.busy": "2025-06-09T05:24:47.736987Z",
     "iopub.status.idle": "2025-06-09T05:24:47.858040Z",
     "shell.execute_reply": "2025-06-09T05:24:47.857284Z",
     "shell.execute_reply.started": "2025-06-09T05:24:47.737218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Disable wanddb tracking\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Logging in to huggingface for training (Save your HF API Token in secrets first)\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=secret_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we can define various training parameters as well as model hosting parameters.\n",
    "\n",
    "We can also optinally calculate additional evaluation score like bertscore during training, but we'll do a comprehensive evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:47.859664Z",
     "iopub.status.busy": "2025-06-09T05:24:47.858964Z",
     "iopub.status.idle": "2025-06-09T05:24:50.637033Z",
     "shell.execute_reply": "2025-06-09T05:24:50.636147Z",
     "shell.execute_reply.started": "2025-06-09T05:24:47.859640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_model_name, # model name in hub when pushed\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    eval_strategy = \"epoch\", # to do evaluation at the end of each epoch\n",
    "    save_strategy = \"epoch\", # saves checkpoint (weights) at the end of each epoch\n",
    "    num_train_epochs = num_epochs, \n",
    "    weight_decay = 0.1,\n",
    "    learning_rate = learning_rate,\n",
    "    fp16 = True, # by default it uses 32bit precision but we can reduce the precision this way\n",
    "    push_to_hub = True, # this will push the model to hub\n",
    "    report_to = \"tensorboard\" # this will report the metrics in hub tensorboard (including additional metrics like bertscore if added)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    args = args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"test\"],\n",
    "    # compute_metrics = compute_metrics # Activate it when using any additonal metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model.\n",
    "\n",
    "Please note that it will run <code> num_epochs x (training_size/ batch_size)</code> steps. So, be mindful about available resources and num_epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T05:24:50.638183Z",
     "iopub.status.busy": "2025-06-09T05:24:50.637946Z",
     "iopub.status.idle": "2025-06-09T06:04:34.936302Z",
     "shell.execute_reply": "2025-06-09T06:04:34.935319Z",
     "shell.execute_reply.started": "2025-06-09T05:24:50.638161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This will complete the training for given number of epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T06:04:34.937495Z",
     "iopub.status.busy": "2025-06-09T06:04:34.937141Z",
     "iopub.status.idle": "2025-06-09T06:04:36.506120Z",
     "shell.execute_reply": "2025-06-09T06:04:36.505431Z",
     "shell.execute_reply.started": "2025-06-09T06:04:34.937469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This will allow pushing to hub\n",
    "trainer.push_to_hub(commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have pushed the model to https://huggingface.co/DipanjanSanyal/wikipedia_sample_tiny_gpt2_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T06:04:36.507149Z",
     "iopub.status.busy": "2025-06-09T06:04:36.506933Z",
     "iopub.status.idle": "2025-06-09T06:04:38.499386Z",
     "shell.execute_reply": "2025-06-09T06:04:38.498525Z",
     "shell.execute_reply.started": "2025-06-09T06:04:36.507132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"DipanjanSanyal/\"+output_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T06:04:38.500657Z",
     "iopub.status.busy": "2025-06-09T06:04:38.500327Z",
     "iopub.status.idle": "2025-06-09T06:04:39.133119Z",
     "shell.execute_reply": "2025-06-09T06:04:39.132463Z",
     "shell.execute_reply.started": "2025-06-09T06:04:38.500622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"Going to school is fun and from them to the first day. in 2012, the same time after the school was the following year. the club was a third - year - day and was the team's first team, as a total of the title. in the season,\"}],\n",
       " [{'generated_text': 'I really like pizza as much as I like holidays because by the ability to prevent their second - time - time - time, having a few years, and in the next day of the band\\'s own. in 2004, the band said that \" the game he was to be \" the song \".'}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    [\n",
    "    \"Going to school is fun and\", \n",
    "    \"I really like pizza as much as I like holidays because\"\n",
    "    ],\n",
    "    # temperature = 0.6,\n",
    "    # truncation = True,\n",
    "    max_new_tokens = response_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model actually did not learn much, but because of a much better tokenizer than what we used in pure pytorch version, we are getting atleast some english word pair or triplets.\n",
    "\n",
    "Looking at the model card details (and tensorboard) you'll notice that, I had also attempted a richer training, but I paused that to save GPU time. By the time I paused, it had run 15 epochs through much more chunks from the same data. Let's also see how that model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74ec27537af4b558b7e8faa72f7ecc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/18.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65434bb139d463ca1e2afadc5d8b15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "pipe2 = pipeline(\"text-generation\", model=\"DipanjanSanyal/\"+output_model_name, \n",
    "                 revision = '87d9aa1eb492a5c20db562f113f07b8f8522f5d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Going to school is fun ands. the first song is played by a different team, a band of young boys in the series before the 1990s, and has been performed by the british band. the band also appeared in several countries to play in the top 40 charts as well as'}],\n",
       " [{'generated_text': 'I really like pizza as much as I like holidays because and other friends of the world. \" it is a popular destination for the city in washington.. after a short period, she has been in the role of the \" most influential woman \", but as a result of the death of an american writer'}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2(\n",
    "    [\n",
    "    \"Going to school is fun and\", \n",
    "    \"I really like pizza as much as I like holidays because\"\n",
    "    ],\n",
    "    # temperature = 0.6,\n",
    "    # truncation = True,\n",
    "    max_new_tokens = response_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model generates much more sensible text, which seems to have been generated from similar articles. But how the model related the input to those set of articles is unclear (how going school relates to music/ british band and how holiday & pizza relates to washington travel destination and some influential woman).\n",
    "\n",
    "In any case, hopefully, we can imagine how good the model becomes as we continue to train it for longer period. We just used a miniscule proportion of wikipedia, which is also a miniscule proportion of internet; and we used a miniscule 4.5M parameter model. If this is done on internet text with billions of parameters, it can generate awesome contextual text, which we see in modern LLMs."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
