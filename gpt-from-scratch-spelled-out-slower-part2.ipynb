{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7fb77c",
   "metadata": {},
   "source": [
    "`Adapted from ng-video-lecture by Andrej Karpathy`\n",
    "\n",
    "`Original source: https://github.com/karpathy/ng-video-lecture`\n",
    "\n",
    "`Simplified and modified for clarity and educational use`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1da28",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005559,
     "end_time": "2025-05-18T04:30:49.120131",
     "exception": false,
     "start_time": "2025-05-18T04:30:49.114572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Recap - Part1\n",
    "\n",
    "We start with the data and the model (without transformer) established in last section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89cf0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:49.130511Z",
     "iopub.status.busy": "2025-05-18T04:30:49.130217Z",
     "iopub.status.idle": "2025-05-18T04:30:53.598801Z",
     "shell.execute_reply": "2025-05-18T04:30:53.597926Z"
    },
    "papermill": {
     "duration": 4.475665,
     "end_time": "2025-05-18T04:30:53.600430",
     "exception": false,
     "start_time": "2025-05-18T04:30:49.124765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcf1207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.610936Z",
     "iopub.status.busy": "2025-05-18T04:30:53.610519Z",
     "iopub.status.idle": "2025-05-18T04:30:53.638258Z",
     "shell.execute_reply": "2025-05-18T04:30:53.637539Z"
    },
    "papermill": {
     "duration": 0.0343,
     "end_time": "2025-05-18T04:30:53.639651",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.605351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################## DATA ######################################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "B = 4\n",
    "T = 6\n",
    "V = 100\n",
    "\n",
    "# Suppose this is a batch of sequences randomly picked from the whole sequence\n",
    "x = np.random.randint(0, V-1, (B, T))\n",
    "\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "inputs = x[:,:-1] # predictor sequences\n",
    "targets = x[:,1:] # predictable sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5db69e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.649443Z",
     "iopub.status.busy": "2025-05-18T04:30:53.649160Z",
     "iopub.status.idle": "2025-05-18T04:30:53.779407Z",
     "shell.execute_reply": "2025-05-18T04:30:53.778486Z"
    },
    "papermill": {
     "duration": 0.137194,
     "end_time": "2025-05-18T04:30:53.781224",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.644030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_logits.shape:  torch.Size([4, 5, 100])\n",
      "\n",
      "Sum of softmax logits of first token in the first sequence as probability:\n",
      " tensor(1., grad_fn=<SumBackward0>)\n",
      "\n",
      "loss:  tensor(4.6062, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "############################  MODEL  ####################################\n",
    "\n",
    "C = 10\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(V, C)\n",
    "        self.position_embedding_table = nn.Embedding(T-1, C)\n",
    "        self.lm_head = nn.Linear(C, V)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, t = idx.shape\n",
    "        x_tok_embed = self.token_embedding_table(idx)\n",
    "        x_pos_embed = self.position_embedding_table(torch.arange(t))\n",
    "        x_embed = x_tok_embed + x_pos_embed\n",
    "        x_t_out = x_embed / 2 # Placeholder for Transformer Blocks\n",
    "        x_raw_logits = self.lm_head(x_t_out)\n",
    "        x_logits = torch.softmax(x_raw_logits, axis=2)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, t, C = x_logits.shape\n",
    "            logits = x_logits.view(B*t, C)\n",
    "            targets = targets.contiguous().view(B*t)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "        #############################################\n",
    "\n",
    "        return x_logits, loss\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Call instance of our model class\n",
    "model = GPTLanguageModel()\n",
    "\n",
    "# Pass predictor sequence through the model\n",
    "x_logits, loss = model(idx = inputs, targets = targets)\n",
    "\n",
    "print(\"x_logits.shape: \", x_logits.shape)\n",
    "print(\"\\nSum of softmax logits of first token in the first sequence as probability:\\n\", torch.sum(x_logits[0,0,:]))\n",
    "print(\"\\nloss: \", loss)"
   ]
  },
  {
   "attachments": {
    "a1351f4f-fee9-4197-9b51-fcbd699a0238.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a1bfe989",
   "metadata": {
    "papermill": {
     "duration": 0.003959,
     "end_time": "2025-05-18T04:30:53.789702",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.785743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Overall Model Structure\n",
    "\n",
    "![Screenshot 2025-05-06 151208.png](attachment:a1351f4f-fee9-4197-9b51-fcbd699a0238.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4beb7b",
   "metadata": {
    "papermill": {
     "duration": 0.003898,
     "end_time": "2025-05-18T04:30:53.797718",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.793820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's first understand **Head**, rest of the stuff in the block are straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c061f",
   "metadata": {
    "papermill": {
     "duration": 0.003804,
     "end_time": "2025-05-18T04:30:53.805698",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.801894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### An Attention Head\n",
    "\n",
    "For this section, let's refer to original transformer paper 'Figure 2' 'Scaled Dot Product Attention'. \n",
    "* We create 3 linear networks `Query(x_embed)`, `Key(x_embed)` and `Value(x_embed)`.\n",
    "* Then we perform matrix multiplication of `Query(x_embed)` and `Key(x_embed)` to create $(T x T)$ attention matrix.\n",
    "* Each element in attention matrix represents how much attention a token should provide to other tokens.\n",
    "* Note that we apply a causal mask (triangular) to make sure that each token attends to previous tokens only.\n",
    "* Further, we multiply this with `Value(x_embed)` to get self-attended representation of the sequence.\n",
    "\n",
    "This is where the tokens are talking to each other and the context is being learnt. We have entered the positional encoding earlier because the $(t \\times t)$ attention matrix will be learnt corresponding to token position. In fact, using the actual token position in sentence would be better than their position in randomly chunked sequence, but it has not been done in GPT for some reason still unclear to me.\n",
    "\n",
    "Let's take the sequence `[\"I\", \"am\", \"feeling\", \"terribly\"]`. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "I \\\\\n",
    "am \\\\\n",
    "feeling \\\\\n",
    "terribly \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Embedding (t x C) - \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.49 & 0.31 & 0.51 & 0.55 & 0.37 & 0.23 & 0.15 & 0.83 & 0.28 & 0.79 \\\\\n",
    "0.92 & 0.62 & 0.65 & 0.53 & 0.53 & 0.03 & 0.47 & 0.77 & 0.35 & 0.63 \\\\\n",
    "0.73 & 0.57 & 0.91 & 0.27 & 0.24 & 0.81 & 0.12 & 0.56 & 0.36 & 0.86 \\\\\n",
    "0.65 & 0.92 & 0.09 & 0.03 & 0.92 & 0.79 & 0.10 & 0.53 & 0.85 & 0.75 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To infer the next token, when we pass this sequence through a trained attention head, it calculates (4 x 4) attention matrix based on the trained networks `Query(x_embed)` and `Key(x_embed)`.\n",
    "\n",
    "`Query(x_embed)` x `Key(x_embed)`$^T$ (t x t) -\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2.51 & 2.77 & 2.69 & 2.49 \\\\\n",
    "2.77 & 3.54 & 3.06 & 2.98 \\\\\n",
    "2.69 & 3.06 & 3.67 & 3.21 \\\\\n",
    "2.49 & 2.98 & 3.21 & 4.34 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Causal Self-Attention (t x t) before softmax -\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2.51 & -inf & -inf & -inf \\\\\n",
    "2.77 & 3.54 & -inf & -inf \\\\\n",
    "2.69 & 3.06 & 3.67 & -inf \\\\\n",
    "2.49 & 2.98 & 3.21 & 4.34 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we apply softmax and multiply with `Value(x_embed)` ( so that -inf becomes 0 and don't contribute anything).\n",
    "\n",
    "The output of a head is (B x t x hs), where hs stands for head_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6624c90f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.815582Z",
     "iopub.status.busy": "2025-05-18T04:30:53.814772Z",
     "iopub.status.idle": "2025-05-18T04:30:53.821682Z",
     "shell.execute_reply": "2025-05-18T04:30:53.821024Z"
    },
    "papermill": {
     "duration": 0.013318,
     "end_time": "2025-05-18T04:30:53.823009",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.809691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, hs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Query, Key and Value trio\n",
    "        self.key = nn.Linear(C, hs, bias=False)\n",
    "        self.query = nn.Linear(C, hs, bias=False)\n",
    "        self.value = nn.Linear(C, hs, bias=False)\n",
    "\n",
    "        # Attention mask (buffer stays in model parameter set but not trainable)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T-1, T-1)))\n",
    "\n",
    "    def forward(self, x_embed):\n",
    "        \n",
    "        B,t,C = x_embed.shape\n",
    "        k = self.key(x_embed)   # (B,t,hs)\n",
    "        q = self.query(x_embed) # (B,t,hs)\n",
    "        v = self.value(x_embed) # (B,t,hs)\n",
    "        \n",
    "        # Compute attention matrix (Qt' scaled by sqrt(C))\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, t, t)\n",
    "\n",
    "        # Replace attention scores with -Inf based on attention mask\n",
    "        wei = wei.masked_fill(self.tril[:t, :t] == 0, float('-inf')) # (B, t, t)\n",
    "\n",
    "        # Softmax\n",
    "        wei = nn.functional.softmax(wei, dim=-1) # (B, t, t)\n",
    "        \n",
    "        # Calculate self-attended representation of the B sequences\n",
    "        head_out = wei @ v # (B, t, hs)\n",
    "        \n",
    "        return head_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0958208b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.832817Z",
     "iopub.status.busy": "2025-05-18T04:30:53.832232Z",
     "iopub.status.idle": "2025-05-18T04:30:53.850071Z",
     "shell.execute_reply": "2025-05-18T04:30:53.848891Z"
    },
    "papermill": {
     "duration": 0.024223,
     "end_time": "2025-05-18T04:30:53.851565",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.827342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5])\n",
      "torch.Size([4, 4, 5])\n",
      "torch.Size([4, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "head = Head(hs=5)\n",
    "\n",
    "# Let's test it for sequences of varying length upto T-1\n",
    "for t in [T-3, T-2, T-1]:\n",
    "    x_embed = torch.rand(B, t, C)\n",
    "    head_out = head(x_embed)\n",
    "    print(head_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136b9f0",
   "metadata": {},
   "source": [
    "#### Discussuon: Why does it work for inferring on shorter sequence?\n",
    "\n",
    "We are creating the attention head which is compatiable with training (T-1) length sequences and inferring next token for upto (T-1) length sequences. \n",
    "\n",
    "A trained self-attention matrix is of dimension (T-1) x (T-1). Each position in the self-attention matrix learns how much it should attend to previous token, previous to previous tokens and so on. If we have a shorter sequence, say t, only the corresponding upper left (t x t) corner of the matrix is used. It doesn't violate any learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bab721",
   "metadata": {
    "papermill": {
     "duration": 0.004013,
     "end_time": "2025-05-18T04:30:53.860110",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.856097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "This is all about -\n",
    "* computing `num_heads` heads in parallel\n",
    "* concatenating the last dim (hs) from all the heads\n",
    "* projecting it to C dim using a `nn.Linear()` netowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5da10a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.869678Z",
     "iopub.status.busy": "2025-05-18T04:30:53.869386Z",
     "iopub.status.idle": "2025-05-18T04:30:53.874820Z",
     "shell.execute_reply": "2025-05-18T04:30:53.874137Z"
    },
    "papermill": {
     "duration": 0.012048,
     "end_time": "2025-05-18T04:30:53.876333",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.864285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, hs):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(hs) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(hs * num_heads, C)\n",
    "\n",
    "    def forward(self, x_embed):\n",
    "        out = torch.cat([h(x_embed) for h in self.heads], dim=-1)\n",
    "        mh_out = self.proj(out)\n",
    "        return mh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744e33be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.887022Z",
     "iopub.status.busy": "2025-05-18T04:30:53.886702Z",
     "iopub.status.idle": "2025-05-18T04:30:53.904451Z",
     "shell.execute_reply": "2025-05-18T04:30:53.903466Z"
    },
    "papermill": {
     "duration": 0.026023,
     "end_time": "2025-05-18T04:30:53.906804",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.880781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 10])\n",
      "torch.Size([4, 4, 10])\n",
      "torch.Size([4, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "multihead = MultiHeadAttention(num_heads = 2, hs = 5)\n",
    "\n",
    "# Let's test it for sequences of varying length upto T-1\n",
    "for t in [T-3, T-2, T-1]:\n",
    "    x_embed = torch.rand(B, t, C)\n",
    "    mh_out = multihead(x_embed)\n",
    "    print(mh_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8397f",
   "metadata": {
    "papermill": {
     "duration": 0.007831,
     "end_time": "2025-05-18T04:30:53.920350",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.912519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feedforward\n",
    "\n",
    "This is the feedforward block attached to MultiHead Attention in the image above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81e3cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.936709Z",
     "iopub.status.busy": "2025-05-18T04:30:53.935778Z",
     "iopub.status.idle": "2025-05-18T04:30:53.942995Z",
     "shell.execute_reply": "2025-05-18T04:30:53.942114Z"
    },
    "papermill": {
     "duration": 0.016463,
     "end_time": "2025-05-18T04:30:53.944685",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.928222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, 4 * C),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * C, C)\n",
    "        )\n",
    "\n",
    "    def forward(self, mh_out):\n",
    "        ff_out = self.net(mh_out)\n",
    "        return ff_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613a008d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.961320Z",
     "iopub.status.busy": "2025-05-18T04:30:53.960683Z",
     "iopub.status.idle": "2025-05-18T04:30:53.971787Z",
     "shell.execute_reply": "2025-05-18T04:30:53.970908Z"
    },
    "papermill": {
     "duration": 0.021022,
     "end_time": "2025-05-18T04:30:53.973552",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.952530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 10])\n",
      "torch.Size([4, 4, 10])\n",
      "torch.Size([4, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "ff = FeedFoward(C = 10)\n",
    "\n",
    "# Let's test it for sequences of varying length upto T-1\n",
    "for t in [T-3, T-2, T-1]:\n",
    "    mh_out = torch.rand(B, t, C)\n",
    "    ff_out = ff(mh_out)\n",
    "    print(ff_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1af5fd",
   "metadata": {
    "papermill": {
     "duration": 0.007378,
     "end_time": "2025-05-18T04:30:53.988642",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.981264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Block\n",
    "\n",
    "A chain of Multi-Head Attention and Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b07ea23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:53.999923Z",
     "iopub.status.busy": "2025-05-18T04:30:53.999044Z",
     "iopub.status.idle": "2025-05-18T04:30:54.005781Z",
     "shell.execute_reply": "2025-05-18T04:30:54.004920Z"
    },
    "papermill": {
     "duration": 0.01403,
     "end_time": "2025-05-18T04:30:54.007464",
     "exception": false,
     "start_time": "2025-05-18T04:30:53.993434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, C, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        hs = C // num_heads # because in the end the heads will be concatenated to C\n",
    "        self.multihead = MultiHeadAttention(num_heads, hs)\n",
    "        self.ff = FeedFoward(C)\n",
    "\n",
    "    def forward(self, x_embed):\n",
    "        mh_out = x_embed + self.multihead(x_embed)\n",
    "        block_out = mh_out + self.ff(mh_out)\n",
    "        return block_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7597c14d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:54.023837Z",
     "iopub.status.busy": "2025-05-18T04:30:54.023453Z",
     "iopub.status.idle": "2025-05-18T04:30:54.034031Z",
     "shell.execute_reply": "2025-05-18T04:30:54.033210Z"
    },
    "papermill": {
     "duration": 0.020415,
     "end_time": "2025-05-18T04:30:54.035558",
     "exception": false,
     "start_time": "2025-05-18T04:30:54.015143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 10])\n",
      "torch.Size([4, 4, 10])\n",
      "torch.Size([4, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "block = Block(C = 10, num_heads = 2)\n",
    "\n",
    "# Let's test it for sequences of varying length upto T-1\n",
    "for t in [T-3, T-2, T-1]:\n",
    "    x_embed = torch.rand(B, t, C)\n",
    "    block_out = block(x_embed)\n",
    "    print(block_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3e59b",
   "metadata": {
    "papermill": {
     "duration": 0.00575,
     "end_time": "2025-05-18T04:30:54.046172",
     "exception": false,
     "start_time": "2025-05-18T04:30:54.040422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "We already had the generative model `GPTLanguageModel` without the `Block`s. Now that we have the blocks. Let's insert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a1f479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T04:30:54.062409Z",
     "iopub.status.busy": "2025-05-18T04:30:54.062096Z",
     "iopub.status.idle": "2025-05-18T04:30:54.083408Z",
     "shell.execute_reply": "2025-05-18T04:30:54.082583Z"
    },
    "papermill": {
     "duration": 0.03182,
     "end_time": "2025-05-18T04:30:54.085110",
     "exception": false,
     "start_time": "2025-05-18T04:30:54.053290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_logits.shape:  torch.Size([4, 5, 100])\n",
      "\n",
      "Sum of softmax logits of first token in the first sequence as probability:\n",
      " tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "\n",
      "loss:  tensor(4.6045, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "############################## DATA ######################################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "B = 4\n",
    "T = 6\n",
    "V = 100\n",
    "\n",
    "# Suppose this is a batch of sequences randomly picked from the whole sequence\n",
    "x = np.random.randint(0, V-1, (B, T))\n",
    "\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "inputs = x[:,:-1] # predictor sequences\n",
    "targets = x[:,1:] # predictable sequences\n",
    "\n",
    "\n",
    "\n",
    "############################  MODEL  ####################################\n",
    "\n",
    "C = 10\n",
    "num_heads = 2 # MODIFICATION : How many heads in MultiHeadAttention?\n",
    "n_layer = 3 # MODIFICATION : How many blocks stacked sequentially?\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(V, C)\n",
    "        self.position_embedding_table = nn.Embedding(T-1, C)\n",
    "        self.blocks = nn.Sequential(*[Block(C, num_heads) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(C, V)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, t = idx.shape\n",
    "        x_tok_embed = self.token_embedding_table(idx)\n",
    "        x_pos_embed = self.position_embedding_table(torch.arange(t))\n",
    "        x_embed = x_tok_embed + x_pos_embed\n",
    "        x_t_out = self.blocks(x_embed) # MODIFICATION : appplying the transformer blocks\n",
    "        x_raw_logits = self.lm_head(x_t_out)\n",
    "        x_logits = torch.softmax(x_raw_logits, axis=2)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, t, C = x_logits.shape\n",
    "            logits = x_logits.view(B*t, C)\n",
    "            targets = targets.contiguous().view(B*t)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "        #############################################\n",
    "\n",
    "        return x_logits, loss\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Call instance of our model class\n",
    "model = GPTLanguageModel()\n",
    "\n",
    "# Pass predictor sequence through the model\n",
    "x_logits, loss = model(idx = inputs, targets = targets)\n",
    "\n",
    "print(\"x_logits.shape: \", x_logits.shape)\n",
    "print(\"\\nSum of softmax logits of first token in the first sequence as probability:\\n\", torch.sum(x_logits[0,0,:]))\n",
    "print(\"\\nloss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09357ce",
   "metadata": {},
   "source": [
    "### Inference/ Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfef86a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence (Prompt): \n",
      " tensor([[46, 85,  6],\n",
      "        [73, 71, 51],\n",
      "        [82, 21,  1],\n",
      "        [59, 65, 57]])\n",
      "\n",
      "\n",
      "Here are the sequences with 10 next tokens predicted\n",
      " tensor([[46, 85,  6, 74, 66, 19, 12, 37, 92, 72, 72, 72, 72],\n",
      "        [73, 71, 51, 15, 66, 19, 19, 19, 19, 19, 19, 19, 19],\n",
      "        [82, 21,  1, 74, 66, 19, 19, 19, 19, 19, 19, 19, 19],\n",
      "        [59, 65, 57, 88,  3, 72, 72, 72, 72, 72, 72, 72, 72]])\n"
     ]
    }
   ],
   "source": [
    "output_tokens = 10 # How many new tokens need to be generated\n",
    "\n",
    "result = torch.randint(0,99,(4,3))\n",
    "print(\"Input Sequence (Prompt): \\n\", result)\n",
    "\n",
    "for i in range(output_tokens):\n",
    "\n",
    "    new_inputs = result[:, -(T-1):]\n",
    "    x_logits, _ = model(idx = new_inputs, targets = None)\n",
    "    next_token_probability = x_logits[:,-1,:] # because we only need to generate last token\n",
    "    predicted_next_token = torch.argmax(next_token_probability, axis = 1)\n",
    "    result = torch.cat((result, predicted_next_token.unsqueeze(1)), dim=1)\n",
    "\n",
    "print(\"\\n\\nHere are the sequences with {} next tokens predicted\\n\".format(output_tokens), \n",
    "      result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.553634,
   "end_time": "2025-05-18T04:30:56.648868",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-18T04:30:44.095234",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
